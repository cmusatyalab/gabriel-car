This is an in-depth overview of the flow of AAA.
------------------------------------------------------------------------------------------------------------
Section.1 - How to use AAA
-First, run start_demo.sh
-Then, run car script on another terminal
------------------------------------------------------------------------------------------------------------
Section.2 - start_demo.sh
The start_demo script starts three servers: gabriel-control, gabriel-ucomm, and twistd

gabriel-control & gabriel-ucomm: It is unclear about what each server does. However, both servers can 
be treated as a single thing that pumps a video stream from the client to the proxy server(car script)
twisd: This server contains all the video resources(we're using a server to store the video resources
because github can't store big files like videos). Videos are stored in port 9095.
------------------------------------------------------------------------------------------------------------
Section.3 - car script
The car script starts a proxy server. The main focus of this server is the handle function. The handle
function is where the raw data of each image received and store in the variable "data". The data is decoded
back into an image via opencv. Then, the image is used by the car_task.py wrapper(explanations on car_task
in section 4). car_task.py will process the image and return the instructions that the client will receive.
After the instructions are received in the handle function, it wraps everything into a json object and 
returns that back to the client.
------------------------------------------------------------------------------------------------------------
Sections.4 - car_task.py
The car_task script processes each image and returns the instructions that would be given to the client. 

Section.4.1 - car_task.py: FrameRecorder
FrameRecorder is a deque that stores arrays of bounding box dimensions. Each array of bounding box 
dimensions would be added to the deque after the object is detected from each image or "frame"(explanations 
on object detect on section 5). This allows FrameRecorder to check if a set of frames is stable or not by 
evaluating the Euclidean distance between each pair of consecutive frames.

Section.4.2 - car_task.py: Task
The whole car assembly procedure is started when a new Task class is created(It is created when a client 
connects to the gabriel server via gabriel client android app). There are two notable functions in Task: 
get_objects_by_categories(look at section 5) and get_instruction. 

The get_instruction function receives the image and processes it depending on which state of the procedure
it is on. Each state of the procedure has its own subroutine and they are packaged into neat 
functions that return a dictionary called "out". The out dictionary has all the instructions that it needs
to return to the client and a "next" key which stores a boolean. This key value pair determines if
the program should move on to the next state or not.
------------------------------------------------------------------------------------------------------------
Section.5 - Object Detection via TPOD
For the object detection part of AAA, we relied on a tool called TPOD(Tool for painless object detection). 
After uploading, annotating, and training the data that you wanted, the classifier can be exported into
satyalab's gitlab(Look at section.5.1 for more detail on this process). From there, you can docker pull 
the image of that classifier into the computer you're working on. The classifier works as a web server so 
you can post it an image and confidence values and you'll receive back a response with all the detection 
in the form of bounding boxes.

Section.5.1 - TPOD Process
TPOD Github: https://github.com/cmusatyalab/tpod

1. Login to cloudlet at http://cloudlet001.elijah.cs.cmu.edu:10000 (ask satyalab to make an account)
2. Upload and annotate video on the video tab
3. Select training data and train classifer on the classifier tab
4. Once classifier is trained, export the classifier (exporting the classifier may take some time)
5. Copy the pull command line that pops up at the top on the classifier page and run it on your computer
6. Run the classifer container with this line:

nvidia-docker run -it -p 0.0.0.0:8000:8000 --rm \
--name <container-name> <container-image> /bin/bash run_server.sh  

7. Boom! The classifier server is running on port 8000. You can post parameters to the \detect directory
on port 8000 and get back bounding boxes

Post parameters: image_directory, confidence, format
Example: picture@appleGreen.jpg confidence=0.95 format=box

Response format: The response is an array of dictionaries. Each dictionary is a object detected. Inside
each dictionary, there would keys like "class", "dimensions", and "confidence". The value stored in class
is the name of the object that is detected. The value stored in the confidence is the confidence of the 
detection. The dimensions value is an array of four floats. The first two floats are the x and 
y position of the top-left corner of the bounding box and the last two floats are the y and x position
of the bottom-right corner of the bounding box.

Response examaple: [{"class": "green_apple", "confidence": 0.789, "dimensions": [320.34,503.2,
456.11, 800.33]},{.....]

Section.5.2 - object_detection.py
Sections 5 and 5.1 shows how object detection works using TPOD. It is pretty simple after you started
the classifier server since you just need to post it the image and confidence and you'll get back a 
response with the object detection. Since there are many parts used in AAA, we also had multiple 
containers trained to detect different objects. One problem with the machine(Intel i7-5960X CPU and 
Nvidia GTX 960 GPU) that we're working on is that it is not strong enough to run more than 2 containers 
at a time. Thus, object_detection script is written to start a selected container and stop a container 
when a different one is selected.     
 




  


